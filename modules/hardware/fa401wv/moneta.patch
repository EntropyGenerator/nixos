diff --git a/.gitignore b/.gitignore
index 86a1ba0d9..a88e78534 100644
--- a/.gitignore
+++ b/.gitignore
@@ -183,3 +183,6 @@ sphinx_*/
 
 # Rust analyzer configuration
 /rust-project.json
+
+# Scripts
+/Scripts
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 974d64bf0..b88418712 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -2059,6 +2059,7 @@ void kvm_mmu_init_memslot_memory_attributes(struct kvm *kvm,
 
 void kvm_mmu_after_set_cpuid(struct kvm_vcpu *vcpu);
 void kvm_mmu_reset_context(struct kvm_vcpu *vcpu);
+int reset_cow_unsync_bitmap(struct kvm *kvm, struct kvm_memory_slot *memslot);
 void kvm_mmu_slot_remove_write_access(struct kvm *kvm,
 				      const struct kvm_memory_slot *memslot,
 				      int start_level);
diff --git a/arch/x86/kvm/Kconfig b/arch/x86/kvm/Kconfig
index 278f08194..98d58be63 100644
--- a/arch/x86/kvm/Kconfig
+++ b/arch/x86/kvm/Kconfig
@@ -143,6 +143,12 @@ config KVM_INTEL_TDX
 
 	  If unsure, say N.
 
+config KVM_AGAMOTTO
+	def_bool y
+	bool "KVM extension for Agamotto"
+	help
+	  Extends KVM to support Agamotto hypercalls.
+
 config KVM_AMD
 	tristate "KVM for AMD processors support"
 	depends on KVM && (CPU_SUP_AMD || CPU_SUP_HYGON)
diff --git a/arch/x86/kvm/Makefile b/arch/x86/kvm/Makefile
index c4b8950c7..fd21cbe05 100644
--- a/arch/x86/kvm/Makefile
+++ b/arch/x86/kvm/Makefile
@@ -13,6 +13,7 @@ kvm-$(CONFIG_KVM_IOAPIC) += i8259.o i8254.o ioapic.o
 kvm-$(CONFIG_KVM_HYPERV) += hyperv.o
 kvm-$(CONFIG_KVM_XEN)	+= xen.o
 kvm-$(CONFIG_KVM_SMM)	+= smm.o
+kvm-$(CONFIG_KVM_AGAMOTTO) += agamotto.o
 
 kvm-intel-y		+= vmx/vmx.o vmx/vmenter.o vmx/pmu_intel.o vmx/vmcs12.o \
 			   vmx/nested.o vmx/posted_intr.o vmx/main.o
diff --git a/arch/x86/kvm/agamotto.c b/arch/x86/kvm/agamotto.c
new file mode 100644
index 000000000..ec7aca50d
--- /dev/null
+++ b/arch/x86/kvm/agamotto.c
@@ -0,0 +1,53 @@
+#define pr_fmt(fmt) "agamotto: " fmt
+
+#include "x86.h"
+
+#include <linux/kvm_host.h>
+
+static int kvm_agamotto_hypercall_complete_userspace(struct kvm_vcpu *vcpu)
+{
+	kvm_register_write(vcpu, VCPU_REGS_RAX, vcpu->run->hypercall.ret);
+	return kvm_skip_emulated_instruction(vcpu);
+}
+
+int kvm_agamotto_hypercall(struct kvm_vcpu *vcpu)
+{
+	unsigned long a0, a1, a2, a3, ret;
+
+	a0 = kvm_register_read(vcpu, VCPU_REGS_RBX);
+	a1 = kvm_register_read(vcpu, VCPU_REGS_RCX);
+	a2 = kvm_register_read(vcpu, VCPU_REGS_RDX);
+	a3 = kvm_register_read(vcpu, VCPU_REGS_RSI);
+
+	switch (a0) {
+	case 0:
+		vcpu->run->exit_reason = KVM_EXIT_AGAMOTTO_BEGIN;
+		break;
+	case 1:
+		vcpu->run->exit_reason = KVM_EXIT_AGAMOTTO_END;
+		vcpu->run->hypercall.args[0] = a1;
+		break;
+	case 10:
+		vcpu->run->exit_reason = KVM_EXIT_AGAMOTTO_DEBUG;
+		vcpu->run->hypercall.args[0] = a1;
+		vcpu->run->hypercall.args[1] = a2;
+		vcpu->run->hypercall.args[2] = a3;
+		break;
+	default:
+		ret = -KVM_EPERM;
+		goto out;
+		break;
+	}
+
+	vcpu->arch.complete_userspace_io =
+		kvm_agamotto_hypercall_complete_userspace;
+
+	return 0;
+
+out:
+	kvm_register_write(vcpu, VCPU_REGS_RAX, ret);
+
+	++vcpu->stat.hypercalls;
+	return kvm_skip_emulated_instruction(vcpu);
+}
+EXPORT_SYMBOL_GPL(kvm_agamotto_hypercall);
\ No newline at end of file
diff --git a/arch/x86/kvm/agamotto.h b/arch/x86/kvm/agamotto.h
new file mode 100644
index 000000000..17c65e925
--- /dev/null
+++ b/arch/x86/kvm/agamotto.h
@@ -0,0 +1,6 @@
+#ifndef __ARCH_X86_KVM_AGAMOTTO_H__
+#define __ARCH_X86_KVM_AGAMOTTO_H__
+
+int kvm_agamotto_hypercall(struct kvm_vcpu *vcpu);
+
+#endif
\ No newline at end of file
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 667d66cf7..c67fc95fb 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -1188,10 +1188,16 @@ static void rmap_remove(struct kvm *kvm, u64 *spte)
 	struct kvm_mmu_page *sp;
 	gfn_t gfn;
 	struct kvm_rmap_head *rmap_head;
+	unsigned int index;
 
 	sp = sptep_to_sp(spte);
-	gfn = kvm_mmu_page_get_gfn(sp, spte_index(spte));
+	// gfn = kvm_mmu_page_get_gfn(sp, spte_index(spte));
+	
+	// clear dma active trace marker
+	index = spte - sp->spt;
+	__clear_bit(index, sp->dma_trace_active);
 
+	gfn = kvm_mmu_page_get_gfn(sp, index);
 	/*
 	 * Unlike rmap_add, rmap_remove does not run in the context of a vCPU
 	 * so we have to determine which memslots to use based on context
@@ -4527,6 +4533,20 @@ static bool kvm_arch_setup_async_pf(struct kvm_vcpu *vcpu,
 				  kvm_vcpu_gfn_to_hva(vcpu, fault->gfn), &arch);
 }
 
+int dma_stop_trace(struct kvm_vcpu *vcpu, gpa_t gpa)
+{
+	return 0;
+}
+EXPORT_SYMBOL(dma_stop_trace);
+
+
+// remove ept entry -> this will trigger mmio handling (hopefully)
+int dma_start_trace(struct kvm_vcpu *vcpu, gpa_t gpa)
+{
+	return 0;
+}
+EXPORT_SYMBOL(dma_start_trace);
+
 void kvm_arch_async_page_ready(struct kvm_vcpu *vcpu, struct kvm_async_pf *work)
 {
 	int r;
@@ -7118,6 +7138,11 @@ static void kvm_shadow_mmu_try_split_huge_pages(struct kvm *kvm,
 				  level, level, start, end - 1, true, true, false);
 }
 
+int reset_cow_unsync_bitmap(struct kvm *kvm, struct kvm_memory_slot *memslot)
+{
+	return 0;
+}
+
 /* Must be called with the mmu_lock held in write-mode. */
 void kvm_mmu_try_split_huge_pages(struct kvm *kvm,
 				   const struct kvm_memory_slot *memslot,
diff --git a/arch/x86/kvm/mmu/mmu_internal.h b/arch/x86/kvm/mmu/mmu_internal.h
index ed5c01df2..f44c14d73 100644
--- a/arch/x86/kvm/mmu/mmu_internal.h
+++ b/arch/x86/kvm/mmu/mmu_internal.h
@@ -115,6 +115,8 @@ struct kvm_mmu_page {
 			 * visited this page.
 			 */
 			atomic_t write_flooding_count;
+
+
 		};
 		/*
 		 * Page table page of external PT.
@@ -144,6 +146,10 @@ struct kvm_mmu_page {
 	int clear_spte_count;
 #endif
 
+	// mark pages for later dma tracing once the spte entry is added
+	DECLARE_BITMAP(dma_trace_active, 512);
+	DECLARE_BITMAP(cow_unsync_bitmap, 512);
+
 #ifdef CONFIG_X86_64
 	/* Used for freeing the page asynchronously if it is a TDP MMU page. */
 	struct rcu_head rcu_head;
diff --git a/arch/x86/kvm/mmu/spte.h b/arch/x86/kvm/mmu/spte.h
index 3133f0669..bfb1caab3 100644
--- a/arch/x86/kvm/mmu/spte.h
+++ b/arch/x86/kvm/mmu/spte.h
@@ -296,6 +296,11 @@ static inline bool is_mmio_spte(struct kvm *kvm, u64 spte)
 	       likely(enable_mmio_caching);
 }
 
+static inline bool is_shadow_present_mmio_pte(u64 pte)
+{
+	return (pte != 0);
+}
+
 static inline bool is_shadow_present_pte(u64 pte)
 {
 	return !!(pte & SPTE_MMU_PRESENT_MASK);
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index e6f2e34ec..e6ce2be3a 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -34,6 +34,8 @@
 #include "xen.h"
 #include "smm.h"
 
+#include "agamotto.h"
+
 #include <linux/clocksource.h>
 #include <linux/interrupt.h>
 #include <linux/kvm.h>
@@ -10394,6 +10396,10 @@ int ____kvm_emulate_hypercall(struct kvm_vcpu *vcpu, int cpl,
 	case KVM_HC_CLOCK_PAIRING:
 		ret = kvm_pv_clock_pairing(vcpu, a0, a1);
 		break;
+#endif
+#ifdef CONFIG_KVM_AGAMOTTO
+	case KVM_HC_AGAMOTTO:
+		return kvm_agamotto_hypercall(vcpu);
 #endif
 	case KVM_HC_SEND_IPI:
 		if (!guest_pv_has(vcpu, KVM_FEATURE_PV_SEND_IPI))
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 5bd76cf39..ef7edc51b 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1229,8 +1229,12 @@ enum kvm_mr_change {
 	KVM_MR_DELETE,
 	KVM_MR_MOVE,
 	KVM_MR_FLAGS_ONLY,
+	KVM_MR_FLAGS_NOT_PRESENT,
 };
 
+int dma_start_trace(struct kvm_vcpu *vcpu, gpa_t gpa);
+int dma_stop_trace(struct kvm_vcpu *vcpu, gpa_t gpa);
+
 int kvm_set_internal_memslot(struct kvm *kvm,
 			     const struct kvm_userspace_memory_region2 *mem);
 void kvm_arch_free_memslot(struct kvm *kvm, struct kvm_memory_slot *slot);
diff --git a/include/uapi/linux/kvm.h b/include/uapi/linux/kvm.h
index 52f6000ab..6a8d552cf 100644
--- a/include/uapi/linux/kvm.h
+++ b/include/uapi/linux/kvm.h
@@ -52,6 +52,8 @@ struct kvm_userspace_memory_region2 {
 #define KVM_MEM_READONLY	(1UL << 1)
 #define KVM_MEM_GUEST_MEMFD	(1UL << 2)
 
+#define KVM_MEM_NOT_PRESENT	(1UL << 1)
+
 /* for KVM_IRQ_LINE */
 struct kvm_irq_level {
 	/*
@@ -138,6 +140,10 @@ struct kvm_xen_exit {
 #define KVM_S390_GET_SKEYS_NONE   1
 #define KVM_S390_SKEYS_MAX        1048576
 
+#define KVM_EXIT_AGAMOTTO_BEGIN  100
+#define KVM_EXIT_AGAMOTTO_END    101
+#define KVM_EXIT_AGAMOTTO_DEBUG  110
+
 #define KVM_EXIT_UNKNOWN          0
 #define KVM_EXIT_EXCEPTION        1
 #define KVM_EXIT_IO               2
@@ -1618,3 +1624,9 @@ struct kvm_pre_fault_memory {
 };
 
 #endif /* __LINUX_KVM_H */
+
+
+#define KVM_ENABLE_DMA_TRACE      _IO(KVMIO,   0xc2)
+#define KVM_DISABLE_DMA_TRACE     _IO(KVMIO,   0xc3)
+#define KVM_UPDATE_USER_MEMORY_REGION _IOW(KVMIO, 0xc4, \
+					struct kvm_userspace_memory_region)
diff --git a/include/uapi/linux/kvm_para.h b/include/uapi/linux/kvm_para.h
index 960c7e93d..6747a1be6 100644
--- a/include/uapi/linux/kvm_para.h
+++ b/include/uapi/linux/kvm_para.h
@@ -18,6 +18,8 @@
 #define KVM_EPERM		EPERM
 #define KVM_EOPNOTSUPP		95
 
+#define KVM_HC_AGAMOTTO		20
+
 #define KVM_HC_VAPIC_POLL_IRQ		1
 #define KVM_HC_MMU_OP			2
 #define KVM_HC_FEATURES			3
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index b7a0ae2a7..2e8d52f43 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -1998,6 +1998,72 @@ static bool kvm_check_memslot_overlap(struct kvm_memslots *slots, int id,
 	return false;
 }
 
+int __kvm_update_memory_region(struct kvm *kvm,
+			    const struct kvm_userspace_memory_region2 *mem)
+{
+	int r;
+	unsigned long npages;
+	struct kvm_memory_slot *slot;
+	int as_id, id;
+
+	r = check_memory_region_flags(kvm,mem);
+	if (r)
+		goto out;
+
+	r = -EINVAL;
+	as_id = mem->slot >> 16;
+	id = (u16)mem->slot;
+
+	/* General sanity checks */
+	if (mem->memory_size & (PAGE_SIZE - 1))
+		goto out;
+	if (mem->guest_phys_addr & (PAGE_SIZE - 1))
+		goto out;
+	/* We can read the guest memory with __xxx_user() later on. */
+	if ((id < KVM_USER_MEM_SLOTS) &&
+	    ((mem->userspace_addr & (PAGE_SIZE - 1)) ||
+	     !access_ok((void __user *)(unsigned long)mem->userspace_addr,
+			mem->memory_size)))
+		goto out;
+	if (id >= KVM_MEM_SLOTS_NUM)
+		goto out;
+	if (mem->guest_phys_addr + mem->memory_size < mem->guest_phys_addr)
+		goto out;
+
+	slot = id_to_memslot(__kvm_memslots(kvm, as_id), id);
+	npages = mem->memory_size >> PAGE_SHIFT;
+
+	if (npages > KVM_MEM_MAX_NR_PAGES)
+		goto out;
+
+	//new.userspace_addr = mem->userspace_addr;
+	slot->userspace_addr = mem->userspace_addr;
+	//reset_cow_unsync_bitmap(kvm, slot);
+	//kvm_mmu_slot_remove_write_access(kvm, slot);
+	// we need to remove all mappings because
+	// the host migth trigger a cow
+	// leaving the guest with a stale spt entry
+	kvm_arch_flush_shadow_memslot(kvm, slot);
+
+	return 0;
+
+out:
+	return r;
+}
+EXPORT_SYMBOL_GPL(__kvm_update_memory_region);
+
+int kvm_update_memory_region(struct kvm *kvm,
+			  const struct kvm_userspace_memory_region2 *mem)
+{
+	int r;
+
+	mutex_lock(&kvm->slots_lock);
+	r = __kvm_update_memory_region(kvm, mem);
+	mutex_unlock(&kvm->slots_lock);
+	return r;
+}
+EXPORT_SYMBOL_GPL(kvm_update_memory_region);
+
 static int kvm_set_memory_region(struct kvm *kvm,
 				 const struct kvm_userspace_memory_region2 *mem)
 {
@@ -2144,6 +2210,33 @@ int kvm_set_internal_memslot(struct kvm *kvm,
 }
 EXPORT_SYMBOL_FOR_KVM_INTERNAL(kvm_set_internal_memslot);
 
+static int kvm_vm_ioctl_disable_dma_trace(struct kvm *kvm, gva_t gva)
+{
+	// todo: fix vcpu index
+	if (atomic_read(&kvm->online_vcpus) > 0) {
+		return dma_stop_trace(xa_load(&kvm->vcpu_array, 0), gva);
+	}
+	return -EINVAL;
+}
+
+static int kvm_vm_ioctl_enable_dma_trace(struct kvm *kvm, gva_t gva)
+{
+	// todo: fix vcpu index
+	if (atomic_read(&kvm->online_vcpus) > 0) {
+		return dma_start_trace(xa_load(&kvm->vcpu_array, 0), gva);
+	}
+	return -EINVAL;
+}
+
+static int kvm_vm_ioctl_update_memory_region(struct kvm *kvm,
+					  struct kvm_userspace_memory_region2 *mem)
+{
+	if ((u16)mem->slot >= KVM_USER_MEM_SLOTS)
+		return -EINVAL;
+
+	return kvm_update_memory_region(kvm, mem);
+}
+
 static int kvm_vm_ioctl_set_memory_region(struct kvm *kvm,
 					  struct kvm_userspace_memory_region2 *mem)
 {
@@ -2950,6 +3043,8 @@ static int hva_to_pfn_remapped(struct vm_area_struct *vma,
 	bool write_fault = kfp->flags & FOLL_WRITE;
 	int r;
 
+	write_fault = true;
+
 	/*
 	 * Remapped memory cannot be pinned in any meaningful sense.  Bail if
 	 * the caller wants to pin the page, i.e. access the page outside of
@@ -5157,6 +5252,17 @@ static long kvm_vm_ioctl(struct file *filp,
 	case KVM_CREATE_VCPU:
 		r = kvm_vm_ioctl_create_vcpu(kvm, arg);
 		break;
+	case KVM_UPDATE_USER_MEMORY_REGION: {
+		struct kvm_userspace_memory_region2 kvm_userspace_mem;
+
+		r = -EFAULT;
+		if (copy_from_user(&kvm_userspace_mem, argp,
+						sizeof(kvm_userspace_mem)))
+			goto out;
+
+		r = kvm_vm_ioctl_update_memory_region(kvm, &kvm_userspace_mem);
+		break;
+	}
 	case KVM_ENABLE_CAP: {
 		struct kvm_enable_cap cap;
 
@@ -5201,6 +5307,14 @@ static long kvm_vm_ioctl(struct file *filp,
 		r = kvm_vm_ioctl_set_memory_region(kvm, &mem);
 		break;
 	}
+	case KVM_ENABLE_DMA_TRACE: {
+		r = kvm_vm_ioctl_enable_dma_trace(kvm, arg);
+		break;
+	}
+	case KVM_DISABLE_DMA_TRACE: {
+		r = kvm_vm_ioctl_disable_dma_trace(kvm, arg);
+		break;
+	}
 	case KVM_GET_DIRTY_LOG: {
 		struct kvm_dirty_log log;
 
